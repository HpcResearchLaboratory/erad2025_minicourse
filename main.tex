\documentclass{SBCbookchapter}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lipsum}

\usepackage[brazilian]{babel}

\usepackage[square]{natbib}
\bibliographystyle{plainnat}

\usepackage{minted}
\usepackage{caption}

\captionsetup[listing]{
    justification=centering,
    singlelinecheck=off,
    name=Código,
    font={bf,small},
}

\captionsetup[figure]{
    justification=centering,
    singlelinecheck=off,
    name=Figura,
    font={bf,small},
}

\captionsetup[table]{
    justification=centering,
    singlelinecheck=off,
    name=Tabela,
    font={bf,small},
}

\setminted{
      frame=lines,
      framesep=2mm,
      baselinestretch=1.2,
      bgcolor=white,
      fontsize=\footnotesize,
      linenos
}

\author{Pablo Alessandro Santos Hugen}
\title{Programação de Alto Desempenho em GPUs com C++}

\begin{document}
\maketitle

\begin{abstract}
	Heterogeneous parallel programming has become an essential approach in the field of \emph{HPC}, especially with the growing use of \emph{GPGPUs}. In this context, the \emph{C++} programming language plays a central role. However, the diversity and complexity of the available models and tools present a significant barrier for beginners in the field. Therefore, the goal of this minicourse is to provide an overview of the main tools and parallel programming models for \textit{GPUs} in \emph{C++}, highlighting their characteristics, peculiarities, and applicability.
\end{abstract}

\begin{resumo}
	A programação paralela heterogênea tem se consolidado como uma abordagem essencial na área de \emph{HPC}, especialmente com o uso crescente de \emph{GPGPUs}. Nesse cenário, a linguagem de programação \emph{C++} desempenha um papel central. Contudo, a diversidade e a complexidade dos modelos e ferramentas disponíveis representam uma barreira significativa para iniciantes na área. Assim, o objetivo deste minicurso é fornecer uma visão geral das principais ferramentas e modelos de programação paralela para \textit{GPUs} em \emph{C++}, destacando suas características, peculiaridades e aplicabilidades.
\end{resumo}

\section{Introdução}\label{sec:introducao}

Nos últimos anos, a Computação de Alto Desempenho (\textit{HPC -- High-Performance Computing}) emergiu como uma ferramenta fundamental para a resolução de problemas em diversos campos, desde a simulação de fenômenos físicos \citep{hong2022mg} até sua aplicação em vários subcampos da Inteligência Artificial, como a análise de dados e a aprendizagem de máquina \citep{elsebakhi2015large}. Paralelamente, \citet{barlas2014multicore} reitera que as Unidades de Processamento Gráfico de Propósito Geral (\emph{GPGPUs, General Purpose Graphics Processing Units}) têm desempenhado um papel cada vez mais crítico nessa área, oferecendo capacidades de processamento massivamente paralelo. Consequentemente, a complexidade dos problemas modernos tem expandido continuamente os limites da computação de alto desempenho. Como exemplo, destaca-se o uso recente da aceleração de múltiplas \textit{GPGPUs} no treinamento de Modelos de Linguagem Massivos (\emph{LLMs, Large Language Models}), que utiliza técnicas de paralelismo de dados para escalar o treinamento desses modelos em \emph{clusters} de \textit{GPUs}.

Nesse contexto, a linguagem de programação C++ desempenha um papel central no desenvolvimento de aplicações para HPC, oferecendo um equilíbrio entre abstração de alto nível e controle eficiente sobre os recursos de hardware. Especialmente no caso das \emph{GPUs}, a linguagem serviu como base para a criação de todo o ecossistema de programação paralela para esses aceleradores. Assim, o objetivo deste minicurso é apresentar os principais modelos de programação paralela em \emph{GPUs} disponíveis na linguagem. A Seção 1.2 explica brevemente o modelo de computação das \emph{GPUs} e a Seção 1.3 enumera as alternativas em C++ para programação desses aceleradores.

\section{Modelo de computação das GPUs}\label{sec:gpu}

Um pré-requisito essencial para escrever algoritmos e estruturas de dados eficientes em \emph{GPUs} é compreender sua arquitetura e como ela difere do modelo tradicional das \textit{CPUs}. Estas, projetadas para execução sequencial, priorizam a redução da latência por meio de otimizações como pipeline de instruções, execução fora de ordem, especulativa e caches multinível \citep{hennessy1990computer, hennessy2018new}. Por outro lado, as \textit{GPUs} foram projetadas para alto \textit{throughput} e paralelismo massivo, mesmo com maior latência na execução de instruções \citep{owens2007gpu}. Esse design, influenciado por aplicações em gráficos, computação numérica e aprendizado de máquina, prioriza cálculos de álgebra linear em alta velocidade. Como visto na Figura \ref{fig_cpu_vs_gpu}, enquanto \textit{CPUs} dedicam mais área do chip a caches e \emph{UCs} para reduzir a latência, \textit{GPUs} maximizam o poder de cálculo com um grande número de \textit{ULAs}, sacrificando latência para otimizar \textit{throughput} \citep{owens2007gpu}.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.2]{./images/cpu_gpu_comparsion.png}}
	\caption{\label{fig_cpu_vs_gpu}Comparativo arquitetural entre \textit{CPUs} e \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming} }
\end{figure}


Assim, as \textit{GPUs} toleram altas latências mantendo desempenho superior ao escalonar milhares de \textit{threads} em paralelo. Embora instruções individuais possam ser lentas, a alternância eficiente entre \textit{threads} garante o uso contínuo dos recursos computacionais. Como destacado por \citet{owens2008gpu}, enquanto algumas \textit{threads} aguardam resultados, outras são executadas, maximizando a ocupação das unidades de processamento e o \textit{throughput}. A Figura \ref{fig_gpu_compute_arch} ilustra essa organização e a execução paralela dentro da \textit{GPU}. Internamente, a Unidade de Processamento Gráfico é composta por um conjunto de Multiprocessadores de Fluxo (\textit{SMs, Streaming Multiprocessors}) \citep{owens2007gpu}, com cada \textit{SM} possuindo uma quantidade de memória compartilhada. Esses \textit{SMs} são fundamentais para o desempenho paralelo da \textit{GPU}, permitindo o processamento simultâneo massivo. Por sua vez, cada \emph{SM} é composto por vários núcleos de processamento. Estes núcleos, frequentemente referidos como \textit{threads}, operam de maneira paralela. Importante destacar que dentro de cada \emph{SM}, todas essas \textit{threads} compartilham memória e outros recursos de processamento, facilitando a execução de tarefas paralelas de forma eficiente.


\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.27]{./images/gpu_compute_arch.png}}
	\caption{\label{fig_gpu_compute_arch} Arquitetura do Modelo de computação das \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming}.}
\end{figure}

Esses aceleradores possuem uma hierarquia de memórias complexa e otimizada para desempenho \citep{mei2016dissecting}. A Figura \ref{fig_gpu_mem_arch} ilustra a organização da memória em um \textit{SM}, incluindo registradores privados por \textit{thread} e \textit{caches} de constantes, que exigem declaração explícita para otimização. Cada \textit{SM} possui uma memória compartilhada (\textit{scratchpad}) de baixa latência, útil para reduzir acessos à memória global e sincronizar \textit{threads} \citep{owens2007gpu}. O cache L1 armazena dados acessados com frequência do cache L2, que é compartilhado entre os \textit{SMs} \citep{picchi2015impact}. A memória global externa, como a \textit{DRAM} de alta largura de banda na arquitetura \textit{Hopper} da Nvidia \citep{choquette2023nvidia}, tem alta latência, mas seu impacto é reduzido pelo escalonamento eficiente das \textit{threads} e pelo uso de caches.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.15]{./images/gpu_mem_arch.jpeg}}
	\caption{\label{fig_gpu_mem_arch} Arquitetura de memória das \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming}}
\end{figure}

A Memória Unificada permite que \textit{CPUs} e \textit{GPUs} acessem um mesmo endereço de memória \citep{nvidiaCUDAUnifiedMemory}. O driver \textit{CUDA} e o \textit{hardware} gerenciam automaticamente a migração das páginas de memória quando necessário. Esse mecanismo é vantajoso para aplicações com padrões de acesso dispersos, evitando a necessidade de pré-carregar grandes arrays ou recorrer a acessos de alta latência fora do dispositivo (\textit{Zero Copy}), com suporte a falhas de página, onde apenas as páginas necessárias são migradas.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.3]{./images/cuda_unified_mem.png}}
	\caption{\label{fig_gpu_mem_unified} Arquitetura de Memória Unificada CUDA. Fonte: \citep{nvidiaCUDAProgramming}}
\end{figure}

\section{Modelos de programação paralela em GPUs disponíveis no C++}\label{sec:gpu_models}

As \textit{GPUs}, inicialmente projetadas para processamento gráfico, evoluíram para se tornar componentes essenciais em diversas aplicações computacionais. Esse avanço acelerado gerou um ecossistema fragmentado, repleto de modelos de programação paralela, compiladores e ferramentas, cada um oferecendo diferentes níveis de abstração e otimização.

Nesse contexto, a linguagem \emph{C++} se firmou como a principal escolha para o desenvolvimento em \textit{GPUs}, fornecendo suporte a uma ampla gama de modelos de programação paralela. Sua combinação de flexibilidade e alto desempenho possibilitou a criação de APIs e frameworks que vão desde abordagens de baixo nível, como \textit{Compute Shaders} e CUDA, até soluções mais portáveis e genéricas, como OpenCL, OpenMP e SYCL. Essa diversidade reflete a necessidade de equilibrar controle granular sobre o hardware com portabilidade e facilidade de uso, permitindo atender a diferentes domínios computacionais. As próximas seções exploram esses modelos em detalhes.

\subsection{Compute Shaders}\label{subsec:compute-shaders}

Os \textit{shaders} são pequenos programas executados diretamente na \textit{GPU}, projetados para processar e manipular dados em estágios específicos do \emph{pipeline gráfico}. Os \emph{vertex shaders} transformam as coordenadas de vértices em um espaço tridimensional, enquanto os \emph{fragment shaders} determinam a cor, textura e iluminação de cada pixel antes da renderização final. No entanto, o \emph{pipeline} gráfico tradicional nem sempre é suficiente para atender a demandas computacionais que vão além da renderização, como simulações físicas e processamento de grandes volumes de dados. Para suprir essa necessidade, \emph{APIs} gráficas modernas introduziram os \emph{Compute Shaders}, que operam fora do \emph{pipeline} e permitem a execução de cálculos arbitrários diretamente na \textit{GPU} usando a mesma \emph{API} gráfica. A introdução desse tipo de \emph{shader} possibilitou o uso da \textit{GPU} para tarefas gerais, expandindo suas aplicações para além da renderização gráfica. A Figura \ref{fig_gpu_pipeline} ilustra a estrutura do \emph{pipeline} gráfico do \emph{OpenGL} e a integração dos \emph{compute shaders} em diferentes estágios.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.3]{./images/gpu_pipeline.png}}
	\caption{\label{fig_gpu_pipeline} Estrutura do \emph{pipeline} gráfico e \emph{compute shaders}. Fonte: \citep{opengl2012bof}}
\end{figure}

No \emph{OpenGL}, os \emph{compute shaders} diferem dos demais por não terem entradas ou saídas fixas, exigindo que busquem e armazenem dados manualmente (em texturas, images ou \emph{uniforms}). Eles operam divididos em \emph{work groups}, definidos pelo usuário na execução. Cada grupo contém várias invocações do \emph{shader} (definidas pelo \emph{local size}) que podem se comunicar entre si, mas não com outros grupos. Como a ordem de processamento é indefinida, não há garantia de sequência fixa. Essa estrutura é ideal para tarefas como processamento de imagens, onde cada grupo lida paralelamente com regiões da imagem (como demonstrado no Código~\ref{lst_compute_shader}). Para executar um \emph{compute shader} em C++, é necessário ativá-lo com \texttt{glUseProgram} ou \texttt{glBindProgramPipeline} e então despachar a computação com \texttt{glDispatchCompute}. Esse comando define quantos \emph{work groups} serão executados nas três dimensões. Também é possível despachar a computação de forma indireta, lendo os valores do número de grupos a partir de um \emph{Buffer Object}, usando \texttt{glDispatchComputeIndirect}.


\begin{listing}[!htb]
	\inputminted{glsl}{./code/compute_shader.glsl}
	\inputminted{cpp}{./code/compute_shader.cpp}
	\caption{\label{lst_compute_shader} Exemplo de \emph{compute shader (glsl)} para inverter cores de imagens.}
\end{listing}

\subsection{CUDA e HIP}\label{subsec:cuda-hip}

Lançada pela \emph{NVIDIA} em novembro de 2006, \emph{CUDA (Compute Unified Device Architecture)} é uma plataforma de computação paralela que permite o uso de suas \emph{GPUs} para processamento geral. Ela fornece uma API baseada em C/C++ para que desenvolvedores possam escrever programas que executem de forma massivamente paralela, aproveitando milhares de núcleos disponíveis nas GPUs modernas. Já o \emph{HIP (Heterogeneous-Compute Interface for Portability)}, é a solução equivalente da \emph{AMD}, que além de permitir a programação de \emph{GPUs} da própria \emph{AMD}, também fornece uma camada de compatibilidade para código \emph{CUDA}, facilitando a portabilidade entre os dois fornecedores de \emph{hardware}.

Essas \emph{APIs} são baseadas em \emph{kernels} (código executado na \emph{GPU}), que são funções chamadas pelo código \emph{host} (código executado na \emph{CPU}) e executadas $N$ vezes em paralelo por $N$ diferentes \emph{threads CUDA}. Um \emph{kernel} é definido usando o especificador de declaração $\_\_global\_\_$, e o número de \emph{threads} que o executam para uma determinada chamada é especificado por meio de uma sintaxe específica ($<<<...>>>$). Cada \emph{thread} recebe um \emph{ID} único, acessível dentro do próprio \emph{kernel}.

Como discutido anteriormente na Seção \ref{sec:gpu}, o conceito de \emph{hierarquia de threads e memória} na arquitetura das \emph{GPUs} é essencial para obter um bom desempenho na execução de \emph{kernels}. O Código~\ref{cod:kernel} exemplifica um \emph{kernel} simples de adição de matrizes em \emph{CUDA}. Nele, podemos observar a organização das \emph{threads} em blocos e sua indexação dentro da \emph{grid} de execução. Cada bloco de \emph{threads} é identificado por um índice \emph{(blockIdx.x e blockIdx.y)}, enquanto cada \emph{thread} dentro do bloco possui um índice local \emph{(threadIdx.x e threadIdx.y)}, permitindo calcular sua posição global na grade ao combiná-los com o tamanho do bloco \emph{(blockDim.x e blockDim.y)}.

\begin{listing}[!htb]
	\inputminted{cuda}{./code/matadd.cu}
	\caption{\label{cod:kernel} Kernel de adição de matrizes em \emph{CUDA}.}
\end{listing}

O \emph{nvcc} é o compilador \emph{CUDA} proprietário da \emph{NVIDIA} responsável por traduzir código-fonte contendo \emph{kernels} em código executável para a \emph{GPU}. Ele separa o código \emph{host} do código \emph{device}, compilando o código \emph{device} para \emph{PTX} ou \emph{cubin} e modificando o código \emph{host} para gerenciar as chamadas aos \emph{kernels}. A compilação pode ser feita de forma \emph{offline}, gerando código binário antecipadamente, ou \emph{just-in-time}, onde o código \emph{PTX} é compilado pelo driver durante a execução da aplicação, garantindo compatibilidade com novas arquiteturas. O uso do \emph{nvcc} simplifica esse processo, fornecendo uma interface unificada para a compilação e a geração de código otimizado para \emph{GPUs}.

Todo o runtime \emph{CUDA} exposto pelo compilador é baseado em uma \emph{API} de baixo nível escrita em C, que também pode ser acessada pela aplicação. Essa interface oferece um controle adicional ao expor conceitos como \emph{contextos}, semelhantes a processos do sistema operacional, e \emph{módulos}, que funcionam como bibliotecas carregadas dinamicamente no dispositivo. A maioria das aplicações não utiliza essa \emph{API} de baixo nível, pois não necessita dessa quantidade extra de controle, o que deixa o código mais simples.

\subsection{OpenMP e OpenACC}\label{subsec:openmp}

O OpenMP (\emph{Open Multi-Processing}) é uma \emph{API} para programação paralela em sistemas de memória compartilhada, como \emph{multi-core CPUs}. Por meio de diretivas de compilador (\emph{pragmas}), ele facilita a paralelização de loops e seções de código com mínima modificação no código sequencial, permitindo distribuir o trabalho entre múltiplas \emph{threads}. Além disso, o OpenMP simplifica o acesso à memória compartilhada, um desafio comum em outros ambientes, onde pode levar a \emph{data races} e problemas semelhantes. Versões recentes também introduziram suporte para offloading para GPUs e outros aceleradores, expandindo suas capacidades além das \emph{CPUs}.

Inicialmente desenvolvido pela empresas \emph{Portland Group (PG)}, \emph{Cray} e \emph{NVIDIA}, o OpenACC (\emph{Open Accelerators}), assim como o OpenMP, utiliza diretivas de compilador para paralelizar o código, mas é focado em aceleração com \emph{GPUs} e outros dispositivos especializados. Com o OpenACC, os desenvolvedores podem facilmente transferir trechos de código e para diferentes tipos de aceleradores, aproveitando as especialidades e poder de computação paralela de cada um deles. Embora também ofereça uma abordagem de alto nível, o OpenACC é especialmente voltado para quem deseja acelerar aplicações sem precisar se aprofundar em conceitos de muito baixo nível.

Apesar de ambas as especificações definirem uma \emph{API} declarativa baseada em diretivas de compilação, o OpenACC foi projetado desde o início para plataformas heterogêneas, como \emph{GPUs} e outros aceleradores, enquanto o OpenMP passou a suportar esses dispositivos apenas a partir da versão 4. Também, o \emph{OpenACC} foca em fornecer computação de alto desempenho de maneira simples e abstraída, especialmente para a comunidade científica. A seguir, o Código~\ref{cod:openmpacc} mostra a soma de arrays paralela utilizando \emph{OpenACC} e \emph{OpenMP}.

\begin{listing}[!htb]
	\inputminted{cpp}{./code/vecadd_oacc.cpp}
	\inputminted{cpp}{./code/vecadd_omp.cpp}
	\caption{\label{cod:openmpacc} Exemplo de adição de arrays paralelo usando \emph{OpenMP} e \emph{OpenACC}.}
\end{listing}

Ambos os códigos são compilados com compiladores comuns como o \emph{GCC} e o \emph{Clang}, utilizando as flags específicas: \texttt{-fopenacc} e \texttt{-fopenmp}, respectivamente.


\subsection{OpenCL}\label{subsec:opencl}

A Open Computing Language (\emph{OpenCL}) é uma especificação aberta para programação paralela heterogênea desenvolvida pela \emph{Khronos Group}, permitindo a execução de código em \emph{CPUs}, \emph{GPUs}, e vários outros aceleradores. Desenvolvida para alto desempenho, a \emph{API} oferece portabilidade e controle de baixo nível entre diferentes arquiteturas de hardware e é amplamente utilizada em aplicações que exigem processamento massivo de dados, como simulações científicas e processamento de imagens. O Código \ref{cod:opencl} ilustra o controle de baixo nível ao implementar um kernel para a redução de arrays em \emph{OpenCL}.

\begin{listing}[!htb]
	\inputminted{cpp}{./code/sum_opencl.cpp}
	\caption{\label{cod:opencl} Kernel de redução (soma) de arrays para \emph{OpenCL}.}
\end{listing}

O objetivo dessa \emph{API} é fornecer uma camada de abstração próxima ao hardware, suficientemente flexível para ser utilizada tanto no desenvolvimento direto quanto como destino de compilação por compiladores e \emph{frameworks} de mais alto nível. A Figura \ref{fig:opencl_env} ilustra a importância dessa \emph{API} como destino de \emph{frameworks} como o \emph{SYCL} ou até mesmo como alvo de compiladores como o \emph{clang}, por meio da representação intermediária \emph{SPIR}.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.25]{./images/opencl_env.png}}
	\caption{\label{fig:opencl_env} Ecossistema do \emph{OpenCL}. Fonte: \citep{khronos_opencl}.}
\end{figure}


\subsection{SYCL}\label{subsec:sycl}

Também desenvolvida pelo \emph{Khronos Group}, o \emph{SYCL} é uma especificação multi-plataforma focada na abstração de algoritmos em diferentes tipos de aceleradores, com alta expressividade com mínima modificação no código. Delegar parte do código para aceleradores específicos é comum na computação de alto desempenho, mas exige conhecimento de bibliotecas e modelos de programação específicos para cada hardware. O \emph{SYCL} resolve isso ao fornecer uma abstração comum para a programação heterogênea, maximizando a conformidade com o padrão \emph{C++}. Sua popularidade resultou no surgimento de implementações como \textbf{Open SYCL}, \textbf{neoSYCL}, \textbf{triSYCL} e \textbf{Intel\textsuperscript{\textregistered} oneAPI tools}.

Essas implementações são na maioria das vezes compatíveis, mas variam em recursos devido a diferenças no desenvolvimento e foco arquitetural, porém todas suportam \emph{CPUs} populares. O Código \ref{cod:sycl} mostra um exemplo de adição de arrays paralelo em \emph{GPU} usando o \emph{SYCL} (implementação \emph{oneAPI}). O uso de \emph{filas} de trabalho e \emph{seletores de dispositivo} deixa o códio genérico ao ponto de, se mudarmos o seletor para \texttt{cpu\_selector\_v} ou \texttt{fpga\_selector\_v}  a função executará no respectivo dispositivo (não necessáriamente com o mesmo desempenho, levando em conta as características de cada um).

\begin{listing}[!htb]
	\inputminted{cpp}{./code/vecadd_sycl.cpp}
	\caption{\label{cod:sycl} Exemplo de adição de arrays paralelo usando \emph{SYCL}.}
\end{listing}

O \emph{SYCL} vem evoluindo com a intenção de influenciar a direção do \emph{ISO C++} em torno da computação heterogênea, criando pontos de prova e features que podem ser considerados no contexto da evolução e adoção na linguagem.

\subsection{std::exec: Execução de tarefas assíncronas/heterogêneas diretamente no C++ }\label{sec:stdexec}

A maturidade do \emph{C++} no ecossistema de \textit{HPC} levou ao desenvolvimento de modelos de programação paralela de alto nível diretamente baseados na linguagem, reduzindo o tempo e o esforço necessários para manter e implantar aplicações, ao mesmo tempo em que garante portabilidade e desempenho com uma única base de código \citep{deakin2020evaluating}. Esses modelos têm como objetivo possibilitar a programação paralela heterogênea com desempenho próximo ao nativo \citep{breyer2022comparison}.

O padrão \textit{C++17} introduziu algoritmos paralelos síncronos (bloqueantes) na biblioteca padrão de \emph{C++}, com suporte para \textit{CPUs} multi-core e \textit{GPUs} com desempenho competitivo \citep{lin2022evaluating}. Esse modelo é inerentemente síncrono, bloqueando a execução do programa até que as tarefas paralelas sejam concluídas, limitando o paralelismo concorrente \citep{lin2022evaluating}. Para resolver isso, os esforços estão focados no desenvolvimento do modelo de programação paralela assíncrona (não bloqueante) \textbf{std::exec} para o próximo padrão \textit{C++26}, facilitando a execução assíncrona flexível e eficiente de tarefas usando abstrações como \textit{senders}, \textit{receivers} e \textit{schedulers} \citep{Dominiak2024}. Um exemplo de execução assíncrona nesse modelo pode ser visto no Código~\ref{cod:stdexec}.

\begin{listing}[!htb] \inputminted{cpp}{./code/stdexec_simple.cpp} \caption{\label{cod:stdexec} Exemplo de execução assíncrona com \textbf{std::exec}.} \end{listing}

Nas abordagens tradicionais de programação paralela e concorrente, utiliza-se threads para paralelismo e primitivas de sincronização (como variáveis atômicas e \emph{mutexes}) para evitar interações incorretas, como \emph{race conditions} e \emph{deadlocks}. Podemos pensar nesse modelo como ``desestruturado'', onde é difícil ter um raciocínio local sobre uma tarefa, pois é preciso considerar o programa como um todo para determinar a sincronização necessária. Com isso, esse \emph{framework} adota o conceito de concorrência estruturada, onde um programa pode ser decomposto em tarefas independentes, executadas concorrentemente e coordenadas de forma assíncrona, abstraindo a programação paralela heterogênea dentro de um mesmo conjunto de abstrações. A Figura \ref{fig:senders} ilustra os principais conceitos disponíveis no \emph{framework}.


\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.3]{./images/sender.png}}
	\caption{\label{fig:senders} Framework \emph{std::exec}. Fonte: \citep{cor3ntin2019executors}}
\end{figure}

Um \emph{sender} é uma entidade que descreve uma tarefa concorrente. Essa tarefa possui um único ponto de entrada e um ponto de saída, com três possíveis resultados de finalização: sucesso (com retorno de um valor), erro (exceção) ou cancelamento (sem retorno). Os schedulers abstraem o local de execução dessas tarefas, possibilitando que sejam realizadas em \emph{CPUs}, \emph{GPUs} ou outros dispositivos, otimizando o uso dos recursos disponíveis. Por fim, os receivers são responsáveis por capturar o resultado de um sender e processá-lo conforme o tipo de conclusão. Com esses blocos de construção, é possível desenvolver programas paralelos e concorrentes de forma estruturada, clara e eficiente. A implementação atual do \emph{framewrk} é fornecida pela \emph{NVIDIA} no compilador \emph{nvc++}, parte do \emph{NVIDIA HPC SDK}. Ela inclui \emph{schedulers} otimizados para as suas \emph{GPUs} proprietárias, inclusive com suporte a \emph{multi-GPUs}.

\section{Considerações finais}

A programação paralela em \emph{GPUs} com \emph{C++} é essencial para o desenvolvimento de aplicações de alto desempenho que aproveitem o máximo do hardware moderno, mas requer o domínio de diversos modelos e ferramentas. Este minicurso apresentou as principais abordagens, desde \emph{APIs} de baixo nível, como \emph{CUDA} e \emph{OpenCL}, até soluções mais portáveis, como \emph{SYCL} e \emph{OpenMP}. Cada modelo tem suas vantagens e desafios, permitindo a escolha mais adequada para cada caso de uso específico. Com esse conhecimento, espera-se que os participantes adquiram uma base sólida sobre as alternativas para explorar e desenvolver aplicações eficientes em \emph{GPUs}. Para aprofundar o entendimento sobre o tema, recomenda-se a leitura do clássico trabalho de \citet{barlas2014multicore} e das outras referências citadas, além das páginas de comunidade e documentação das ferramentas apresentadas.



\bibliography{bibliography}

\end{document}
