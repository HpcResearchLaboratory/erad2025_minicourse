\documentclass{SBCbookchapter}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lipsum}

\usepackage[brazilian]{babel}

\usepackage[square]{natbib}
\bibliographystyle{plainnat}

\usepackage{minted}
\usepackage{caption}
\captionsetup[listing]{
    justification=centering,
    singlelinecheck=off,
    name=Código,
    font={bf,small},
}

\captionsetup[figure]{
    justification=centering,
    singlelinecheck=off,
    name=Figura,
    font={bf,small},
}

\captionsetup[table]{
    justification=centering,
    singlelinecheck=off,
    name=Tabela,
    font={bf,small},
}

\setminted{
      frame=lines,
      framesep=2mm,
      baselinestretch=1.2,
      bgcolor=white,
      fontsize=\footnotesize,
      linenos
}

\author{Pablo Alessandro Santos Hugen}

\begin{document}
\maketitle

\begin{abstract}
	This meta-paper describes the style to be used in articles and short
	papers for SBC conferences. For papers in English, you should add just
	an abstract and for the papers in Portuguese, we also ask for an
	abstract in Portuguese (``resumo''). In both cases, abstracts should not
	have more than 10~lines and must be in the first page of the paper.
\end{abstract}

\begin{resumo}
	\begin{otherlanguage}{brazilian}
		Este meta-artigo descreve o estilo a ser usado na confecção de artigos
		e resumos de artigos para publicação nos anais das conferências
		organizadas pela SBC. É solicitada a escrita de resumo e abstract apenas
		para os artigos escritos em português. Artigos em inglês, deverão
		possuir apenas abstract. Nos dois casos, o autor deve tomar cuidado para
		que o resumo (e o abstract) não ultrapassem 10~linhas cada, sendo que
		ambos devem estar na primeira página do artigo.
	\end{otherlanguage}
\end{resumo}

\section{Introdução}\label{sec:introducao}

Nos últimos anos, a Computação de Alto Desempenho (\textit{HPC -- High-Performance Computing}) emergiu como uma ferramenta fundamental para a resolução de problemas em diversos campos, desde a simulação de fenômenos físicos \citep{hong2022mg} até sua aplicação em vários subcampos da Inteligência Artificial, como a análise de dados e a aprendizagem de máquina \citep{elsebakhi2015large}. Paralelamente, \citet{barlas2014multicore} reitera que as Unidades de Processamento Gráfico de Propósito Geral (\emph{GPGPUs, General Purpose Graphics Processing Units}) têm desempenhado um papel cada vez mais crítico nessa área, oferecendo capacidades de processamento massivamente paralelo. Consequentemente, a complexidade dos problemas modernos tem expandido continuamente os limites da computação de alto desempenho. Como exemplo, destaca-se o uso recente da aceleração de múltiplas \textit{GPGPUs} no treinamento de Modelos de Linguagem Massivos (\emph{LLMs, Large Language Models}), que utiliza técnicas de paralelismo de dados para escalar o treinamento desses modelos em \emph{clusters} de \textit{GPUs}.

Nesse contexto, a linguagem de programação C++ desempenha um papel central no desenvolvimento de aplicações para HPC, oferecendo um equilíbrio entre abstração de alto nível e controle eficiente sobre os recursos de hardware. Especialmente no caso das \emph{GPUs}, a linguagem serviu como base para a criação de todo o ecossistema de programação paralela para esses aceleradores. Assim, o objetivo deste minicurso é apresentar os principais modelos de programação paralela em \emph{GPUs} disponíveis para C++, com destaque para o modelo assíncrono e heterogêneo \textbf{stdexec}. A Seção 1.2 explica brevemente o modelo de computação das \emph{GPUs}; a Seção 1.3 enumera as alternativas em C++ para programação desses aceleradores; e, por fim, a Seção 1.4 concentra-se no modelo mais recente, o \emph{stdexec}.


\section{Modelo de computação das GPUs}\label{sec:gpu}

Um pré-requisito essencial para escrever algoritmos e estruturas de dados eficientes em \emph{GPUs} é compreender sua arquitetura e como ela difere do modelo tradicional das \textit{CPUs}. Estas, projetadas para execução sequencial, priorizam a redução da latência por meio de otimizações como pipeline de instruções, execução fora de ordem, especulativa e caches multinível \citep{hennessy1990computer, hennessy2018new}. Por outro lado, as \textit{GPUs} foram projetadas para alto \textit{throughput} e paralelismo massivo, mesmo com maior latência na execução de instruções \citep{owens2007gpu}. Esse design, influenciado por aplicações em gráficos, computação numérica e aprendizado de máquina, prioriza cálculos de álgebra linear em alta velocidade. Como visto na Figura \ref{fig_cpu_vs_gpu}, enquanto \textit{CPUs} dedicam mais área do chip a caches e \emph{UCs} para reduzir a latência, \textit{GPUs} maximizam o poder de cálculo com um grande número de \textit{ULAs}, sacrificando latência para otimizar \textit{throughput} \citep{owens2007gpu}.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.2]{./images/cpu_gpu_comparsion.png}}
	\caption{\label{fig_cpu_vs_gpu}Comparativo arquitetural entre \textit{CPUs} e \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming} }
\end{figure}


Assim, as \textit{GPUs} toleram altas latências mantendo desempenho superior ao escalonar milhares de \textit{threads} em paralelo. Embora instruções individuais possam ser lentas, a alternância eficiente entre \textit{threads} garante o uso contínuo dos recursos computacionais. Como destacado por \citet{owens2008gpu}, enquanto algumas \textit{threads} aguardam resultados, outras são executadas, maximizando a ocupação das unidades de processamento e o \textit{throughput}. A Figura \ref{fig_gpu_compute_arch} ilustra essa organização e a execução paralela dentro da \textit{GPU}. Internamente, a Unidade de Processamento Gráfico é composta por um conjunto de Multiprocessadores de Fluxo (\textit{SMs, Streaming Multiprocessors}) \citep{owens2007gpu}, com cada \textit{SM} possuindo uma quantidade de memória compartilhada. Esses \textit{SMs} são fundamentais para o desempenho paralelo da \textit{GPU}, permitindo o processamento simultâneo massivo. Por sua vez, cada \emph{SM} é composto por vários núcleos de processamento. Estes núcleos, frequentemente referidos como \textit{threads}, operam de maneira paralela. Importante destacar que dentro de cada \emph{SM}, todas essas \textit{threads} compartilham memória e outros recursos de processamento, facilitando a execução de tarefas paralelas de forma eficiente.


\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.27]{./images/gpu_compute_arch.png}}
	\caption{\label{fig_gpu_compute_arch} Arquitetura do Modelo de computação das \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming}.}
\end{figure}

Esses aceleradores possuem uma hierarquia de memórias complexa e otimizada para desempenho \citep{mei2016dissecting}. A Figura \ref{fig_gpu_mem_arch} ilustra a organização da memória em um \textit{SM}, incluindo registradores privados por \textit{thread} e \textit{caches} de constantes, que exigem declaração explícita para otimização. Cada \textit{SM} possui uma memória compartilhada (\textit{scratchpad}) de baixa latência, útil para reduzir acessos à memória global e sincronizar \textit{threads} \citep{owens2007gpu}. O cache L1 armazena dados acessados com frequência do cache L2, que é compartilhado entre os \textit{SMs} \citep{picchi2015impact}. A memória global externa, como a \textit{DRAM} de alta largura de banda na arquitetura \textit{Hopper} da Nvidia \citep{choquette2023nvidia}, tem alta latência, mas seu impacto é reduzido pelo escalonamento eficiente das \textit{threads} e pelo uso de caches.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.15]{./images/gpu_mem_arch.jpeg}}
	\caption{\label{fig_gpu_mem_arch} Arquitetura de memória das \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming}}
\end{figure}

A Memória Unificada permite que \textit{CPUs} e \textit{GPUs} acessem um mesmo endereço de memória \citep{nvidiaCUDAUnifiedMemory}. O driver \textit{CUDA} e o \textit{hardware} gerenciam automaticamente a migração das páginas de memória quando necessário. Esse mecanismo é vantajoso para aplicações com padrões de acesso dispersos, evitando a necessidade de pré-carregar grandes arrays ou recorrer a acessos de alta latência fora do dispositivo (\textit{Zero Copy}), com suporte a falhas de página, onde apenas as páginas necessárias são migradas.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.3]{./images/cuda_unified_mem.png}}
	\caption{\label{fig_gpu_mem_unified} Arquitetura de Memória Unificada CUDA. Fonte: \citep{nvidiaCUDAProgramming}}
\end{figure}

\section{Modelos de programação paralela em GPUs disponíveis no C++}\label{sec:gpu_models}

As \textit{GPUs}, inicialmente projetadas para processamento gráfico, evoluíram para se tornar componentes essenciais em diversas aplicações computacionais. Esse avanço acelerado gerou um ecossistema fragmentado, repleto de modelos de programação paralela, compiladores e ferramentas, cada um oferecendo diferentes níveis de abstração e otimização.

Nesse contexto, a linguagem \emph{C++} se firmou como a principal escolha para o desenvolvimento em \textit{GPUs}, fornecendo suporte a uma ampla gama de modelos de programação paralela. Sua combinação de flexibilidade e alto desempenho possibilitou a criação de APIs e frameworks que vão desde abordagens de baixo nível, como \textit{Compute Shaders} e CUDA, até soluções mais portáveis e genéricas, como OpenCL, OpenMP e SYCL. Essa diversidade reflete a necessidade de equilibrar controle granular sobre o hardware com portabilidade e facilidade de uso, permitindo atender a diferentes domínios computacionais. As próximas seções exploram esses modelos em detalhes, destacando suas características, vantagens e casos de uso.

\subsection{Compute Shaders}\label{subsec:compute-shaders}

Os \textit{shaders} são pequenos programas executados diretamente na \textit{GPU}, projetados para processar e manipular dados em estágios específicos do \emph{pipeline gráfico}. Os \emph{vertex shaders} transformam as coordenadas de vértices em um espaço tridimensional, enquanto os \emph{fragment shaders} determinam a cor, textura e iluminação de cada pixel antes da renderização final. No entanto, o \emph{pipeline} gráfico tradicional nem sempre é suficiente para atender a demandas computacionais que vão além da renderização, como simulações físicas e processamento de grandes volumes de dados. Para suprir essa necessidade, \emph{APIs} gráficas modernas introduziram os \emph{Compute Shaders}, que operam fora do \emph{pipeline} e permitem a execução de cálculos arbitrários diretamente na \textit{GPU} usando a mesma \emph{API} gráfica. A introdução desse tipo de shader possibilitou o uso da \textit{GPU} para tarefas gerais, expandindo suas aplicações para além da renderização gráfica. A Figura \ref{fig_gpu_pipeline} ilustra a estrutura do \emph{pipeline} gráfico do \emph{OpenGL} e a integração dos \emph{compute shaders} em diferentes estágios.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.3]{./images/gpu_pipeline.png}}
	\caption{\label{fig_gpu_pipeline} Estrutura do \emph{pipeline} gráfico e \emph{compute shaders}. Fonte: \citep{opengl2012bof}}
\end{figure}

No \emph{OpenGL}, os \emph{compute shaders} diferem dos demais por não terem entradas ou saídas fixas, exigindo que busquem e armazenem dados manualmente (em texturas, images ou \emph{uniforms}). Eles operam divididos em \emph{work groups}, definidos pelo usuário na execução. Cada grupo contém várias invocações do shader (definidas pelo \emph{local size}) que podem se comunicar entre si, mas não com outros grupos. Como a ordem de processamento é indefinida, não há garantia de sequência fixa. Essa estrutura é ideal para tarefas como processamento de imagens, onde cada grupo lida paralelamente com regiões da imagem. Para executar um \emph{compute shader} em C++, é necessário ativá-lo com \texttt{glUseProgram} ou \texttt{glBindProgramPipeline} e então despachar a computação com \texttt{glDispatchCompute}. Esse comando define quantos \emph{work groups} serão executados nas três dimensões. Também é possível despachar a computação de forma indireta, lendo os valores do número de grupos a partir de um \emph{Buffer Object}, usando \texttt{glDispatchComputeIndirect}.


\begin{listing}[!htb]
	\inputminted{glsl}{./code/compute_shader.glsl}
	\inputminted{cpp}{./code/compute_shader.cpp}
	\caption{\label{lst_compute_shader} Exemplo de \emph{compute shader (glsl)} para inverter cores de imagens.}
\end{listing}

\subsection{CUDA e HIP}\label{subsec:cuda-hip}

Lançada pela \emph{NVIDIA} em novembro de 2006, \emph{CUDA (Compute Unified Device Architecture)} é uma plataforma de computação paralela que permite o uso de suas \emph{GPUs} da para processamento geral. Ela fornece uma API baseada em C/C++ para que desenvolvedores possam escrever programas que executem de forma massivamente paralela, aproveitando milhares de núcleos disponíveis nas GPUs modernas. Já o \emph{HIP (Heterogeneous-Compute Interface for Portability)}, é a solução equivalente da \emph{AMD}, que além de permitir a programação de \emph{GPUs} da própria \emph{AMD}, também fornece uma camada de compatibilidade para código \emph{CUDA}, facilitando a portabilidade entre os dois fornecedores de \emph{hardware}.

Essas \emph{APIs} são baseadas em \emph{kernels} (código executado na \emph{GPU}), que são funções chamadas pelo código \emph{host} (código executado na \emph{CPU}) e executadas $N$ vezes em paralelo por $N$ diferentes \emph{threads CUDA}. Um \emph{kernel} é definido usando o especificador de declaração $\_\_global\_\_$, e o número de \emph{threads} que o executam para uma determinada chamada é especificado por meio de uma sintaxe específica ($<<<...>>>$). Cada \emph{thread} recebe um \emph{ID} único, acessível dentro do próprio \emph{kernel}.

Como discutido anteriormente na Seção \ref{sec:gpu}, o conceito de \emph{hierarquia de threads e memória} na arquitetura das \emph{GPUs} é essencial para obter um bom desempenho na execução de \emph{kernels}. No Código \ref{cod:kernel}, podemos observar a organização das \emph{threads} em blocos e sua indexação dentro da \emph{grid} de execução. Cada bloco de \emph{threads} é identificado por um índice \emph{(blockIdx.x e blockIdx.y)}, enquanto cada \emph{thread} dentro do bloco possui um índice local \emph{(threadIdx.x e threadIdx.y)}, permitindo calcular sua posição global na grade ao combiná-los com o tamanho do bloco \emph{(blockDim.x e blockDim.y)}.

\begin{listing}[!htb]
	\inputminted{cuda}{./code/matadd.cu}
	\caption{\label{cod:kernel} Kernel de adição de matrizes em \emph{CUDA}.}
\end{listing}

O \emph{nvcc} é o compilador \emph{CUDA} proprietário da \emph{NVIDIA} responsável por traduzir código-fonte contendo \emph{kernels} em código executável para a \emph{GPU}. Ele separa o código \emph{host} do código \emph{device}, compilando o código \emph{device} para \emph{PTX} ou \emph{cubin} e modificando o código \emph{host} para gerenciar as chamadas aos \emph{kernels}. A compilação pode ser feita de forma \emph{offline}, gerando código binário antecipadamente, ou \emph{just-in-time}, onde o código \emph{PTX} é compilado pelo driver durante a execução da aplicação, garantindo compatibilidade com novas arquiteturas. O uso do \emph{nvcc} simplifica esse processo, fornecendo uma interface unificada para a compilação e a geração de código otimizado para \emph{GPUs}.

Todo o runtime \emph{CUDA} exposto pelo compilador é baseado em uma \emph{API} de baixo nível escrita em C, que também pode ser acessada pela aplicação. Essa interface oferece um controle adicional ao expor conceitos como \emph{contextos}, semelhantes a processos do sistema operacional -- e \emph{módulos}, que funcionam como bibliotecas carregadas dinamicamente no dispositivo. A maioria das aplicações não utiliza essa \emph{API} de baixo nível, pois não necessita dessa quantidade extra de controle, o que deixa o código mais simples.

\subsection{OpenMP e OpenACC}\label{subsec:openmp}

O OpenMP (\emph{Open Multi-Processing}) é uma \emph{API} para programação paralela em sistemas de memória compartilhada, como \emph{multi-core CPUs}. Por meio de diretivas de compilador (\emph{pragmas}), ele facilita a paralelização de loops e seções de código com mínima modificação no código sequencial, permitindo distribuir o trabalho entre múltiplas \emph{threads}. Além disso, o OpenMP simplifica o acesso à memória compartilhada, um desafio comum em outros ambientes, onde pode levar a \emph{data races} e problemas semelhantes. Versões recentes também introduziram suporte para offloading para GPUs e outros aceleradores, expandindo suas capacidades além das \emph{CPUs}.

Inicialmente desenvolvido pela empresas \emph{Portland Group (PG)}, \emph{Cray} e \emph{NVIDIA}, o OpenACC (\emph{Open Accelerators}), assim como o OpenMP, utiliza diretivas de compilador para paralelizar o código, mas é focado em aceleração com GPUs e outros dispositivos especializados. Com o OpenACC, os desenvolvedores podem facilmente transferir trechos de código e para diferentes tipos de aceleradores, aproveitando as especialidades e poder de computação paralela de cada um deles. Embora também ofereça uma abordagem de alto nível, o OpenACC é especialmente voltado para quem deseja acelerar aplicações sem precisar se aprofundar em conceitos de muito baixo nível.

Apesar de ambas as especificações definirem uma \emph{API} declarativa de alto nível baseada em diretivas de compilação, o OpenACC foi projetado desde o início para plataformas heterogêneas, como GPUs e outros aceleradores, enquanto o OpenMP passou a suportar esses dispositivos apenas a partir da versão 4. Também, o \emph{OpenACC} foca em fornecer computação de alto desempenho de maneira simples e abstraída, especialmente para a comunidade científica. A seguir, o código mostra a soma de arrays paralela utilizando OpenMP e OpenACC.

\begin{listing}[!htb]
	\inputminted{cpp}{./code/vecadd_oacc.cpp}
	\inputminted{cpp}{./code/vecadd_omp.cpp}
	\caption{\label{cod:openmpacc} Exemplo de adição de arrays paralelo usando \emph{OpenMP} e \emph{OpenACC}.}
\end{listing}

Ambos os códigos são compilados com compiladores comuns como o \emph{GCC} e o \emph{Clang}, utilizando as flags específicas: \texttt{-fopenmp} e \texttt{-fopenacc}, respectivamente.



\subsection{OpenCL}\label{subsec:opencl}

A Open Computing Language (\emph{OpenCL}) é uma especificação aberta para programação paralela heterogênea, permitindo a execução de código em \emph{CPUs}, \emph{GPUs}, \emph{DSPs}, \emph{FPGAs} e outros aceleradores. Desenvolvida para alta performance, a \emph{API} oferece portabilidade entre diferentes arquiteturas de hardware e é amplamente utilizada em aplicações que exigem processamento massivo de dados, como simulações científicas e processamento de imagens. Por meio dessa \emph{API}, é possível delegar trechos do código mais intensivos em computação para dispositivos de aceleração adequados.


\subsection{SYCL}\label{subsec:sycl}

\subsection{std::exec}\label{subsec:sdtexec}

A maturidade do \emph{C++} no ecossistema de \textit{HPC} levou ao desenvolvimento de modelos de programação paralela de alto nível diretamente baseados na linguagem, reduzindo o tempo e o esforço necessários para manter e implantar aplicações, ao mesmo tempo em que garante portabilidade e desempenho com uma única base de código \citep{deakin2020evaluating}. Esses modelos têm como objetivo possibilitar a programação paralela heterogênea com desempenho próximo ao nativo \citep{breyer2022comparison}.

O padrão \textit{C++17} introduziu algoritmos paralelos síncronos (bloqueantes) na biblioteca padrão de \emph{C++}, com suporte para \textit{CPUs} multi-core e \textit{GPUs} com desempenho competitivo \citep{lin2022evaluating}. Esse modelo é inerentemente síncrono, bloqueando a execução do programa até que as tarefas paralelas sejam concluídas, limitando o paralelismo concorrente \citep{lin2022evaluating}. Para resolver isso, os esforços estão focados no desenvolvimento do modelo de programação paralela assíncrona (não bloqueante) \textbf{std::exec} para o próximo padrão \textit{C++26}, facilitando a execução assíncrona flexível e eficiente de tarefas usando abstrações como \textit{senders}, \textit{receivers} e \textit{schedulers} \citep{Dominiak2024}.

\begin{listing}[!htb]
	\inputminted{cpp}{./code/stdexec_simple.cpp}
	\caption{\label{cod:stdexec} Exemplo de execução assíncrona com \textbf{std::exec}.}
\end{listing}


\section{std::exec: Execução de tarefas assíncronas/heterogêneas no C++ }\label{sec:stdexec}

\bibliography{bibliography}

\end{document}
