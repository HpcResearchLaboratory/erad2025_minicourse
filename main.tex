\documentclass{SBCbookchapter}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage[brazilian]{babel}

\usepackage[square]{natbib}
\bibliographystyle{plainnat}

\author{Pablo Alessandro Santos Hugen}

\begin{document}
\maketitle

\begin{abstract}
	This meta-paper describes the style to be used in articles and short
	papers for SBC conferences. For papers in English, you should add just
	an abstract and for the papers in Portuguese, we also ask for an
	abstract in Portuguese (``resumo''). In both cases, abstracts should not
	have more than 10~lines and must be in the first page of the paper.
\end{abstract}

\begin{resumo}
	\begin{otherlanguage}{brazilian}
		Este meta-artigo descreve o estilo a ser usado na confecção de artigos
		e resumos de artigos para publicação nos anais das conferências
		organizadas pela SBC. É solicitada a escrita de resumo e abstract apenas
		para os artigos escritos em português. Artigos em inglês, deverão
		possuir apenas abstract. Nos dois casos, o autor deve tomar cuidado para
		que o resumo (e o abstract) não ultrapassem 10~linhas cada, sendo que
		ambos devem estar na primeira página do artigo.
	\end{otherlanguage}
\end{resumo}

\section{Introdução}\label{sec:introducao}

Nos últimos anos, a Computação de Alto Desempenho (\textit{HPC -- High-Performance Computing}) emergiu como uma ferramenta fundamental para a resolução de problemas em diversos campos, desde a simulação de fenômenos físicos \citep{hong2022mg} até sua aplicação em vários subcampos da Inteligência Artificial, como a análise de dados e a aprendizagem de máquina \citep{elsebakhi2015large}. Paralelamente, \citet{barlas2014multicore} reitera que as Unidades de Processamento Gráfico de Propósito Geral (\emph{GPGPUs, General Purpose Graphics Processing Units}) têm desempenhado um papel cada vez mais crítico nessa área, oferecendo capacidades de processamento massivamente paralelo. Consequentemente, a complexidade dos problemas modernos tem expandido continuamente os limites da computação de alto desempenho. Como exemplo, destaca-se o uso recente da aceleração de múltiplas \textit{GPGPUs} no treinamento de Modelos de Linguagem Massivos (\emph{LLMs, Large Language Models}), que utiliza técnicas de paralelismo de dados para escalar o treinamento desses modelos em \emph{clusters} de \textit{GPUs}.

Nesse contexto, a linguagem de programação C++ desempenha um papel central no desenvolvimento de aplicações para HPC, oferecendo um equilíbrio entre abstração de alto nível e controle eficiente sobre os recursos de hardware. Especialmente no caso das \emph{GPUs}, a linguagem serviu como base para a criação de todo o ecossistema de programação paralela para esses aceleradores. Assim, o objetivo deste minicurso é apresentar os principais modelos de programação paralela em \emph{GPUs} disponíveis para C++, com destaque para o modelo assíncrono e heterogêneo \textbf{stdexec}. A Seção 1.2 explica brevemente o modelo de computação das \emph{GPUs}; a Seção 1.3 enumera as alternativas em C++ para programação desses aceleradores; e, por fim, a Seção 1.4 concentra-se no modelo mais recente, o \emph{stdexec}.


\section{Modelo de computação das GPUs}\label{sec:gpu}

Um pré requisito básico para a escrita de algoritmos e estruturas de dados eficientes na programação para \emph{GPUs} é entender a sua arquitetura, e como ela difere do modelo tradicional. A principal diferença entre \textit{CPUs} e \textit{GPUs} reside em seus objetivos de design. As \textit{CPUs} foram projetadas para executar instruções sequenciais. Para aprimorar o desempenho dessa execução sequencial, diversas funcionalidades foram incorporadas ao design das \textit{CPUs} ao longo dos anos \citep{hennessy1990computer}. O foco tem sido a redução da latência na execução de instruções, de modo que as \textit{CPUs} possam processar uma sequência de instruções o mais rápido possível. Como destacado por \citeonline{hennessy2018new}, isso inclui características como: pipeline de instruções, execução fora de ordem, execução especulativa e caches multinível.

Por outro lado, as \textit{GPUs} foram desenvolvidas visando níveis massivos de paralelismo e alto \textit{throughput}, ainda que isso signifique uma latência média a alta na execução de instruções \citep{owens2007gpu}. Essa direção no design foi influenciada por sua utilização em jogos eletrônicos, gráficos, computação numérica e, mais recentemente, em aprendizado de máquina. Todas essas aplicações exigem a realização de uma grande quantidade de cálculos de álgebra linear e computações numéricas em uma velocidade muito elevada, o que levou a um foco significativo no aprimoramento do \textit{throughput} desses dispositivos \cite{owens2007gpu}. Como exemplificado na Figura \ref{fig_cpu_vs_gpu}, as \textit{CPUs} dedicam uma quantidade significativa de área do chip para recursos que reduzem a latência das instruções, como caches grandes, possuem menos \textit{ULAs} (Unidades Logica e Aritmética) e mais unidades de controle. Em contraste, as \textit{GPUs} utilizam um grande número de \textit{ULAs} para maximizar seu poder de cálculo e taxa de transferência. Eles usam uma quantidade muito pequena da área do chip para caches e unidades de controle, decisão de projeto que aumenta a latência média de instruções nas \textit{GPUs}.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.2]{./images/cpu_gpu_comparsion.png}}
	\caption{\label{fig_cpu_vs_gpu}Comparativo arquitetural entre \textit{CPU} (esquerda) e \textit{GPU} (direita). Fonte: \citep{nvidiaCUDAProgramming} }
\end{figure}


\subsubsection{Arquitetura do Modelo de Computação}

Portanto, evidencia-se que as \textit{GPUs} são capazes de tolerar altas latências, mantendo um desempenho superior, graças à sua habilidade de combinar um elevado número de \textit{threads}. Embora as instruções individuais possam apresentar altas latências, as \textit{GPUs} têm a habilidade de escalonar as \textit{threads} para execução de forma eficiente, assegurando a utilização constante do poder computacional disponível. Em seu texto, \citet{owens2008gpu} reforça que enquanto determinadas \textit{threads} estão em um estado de espera pelo resultado de uma instrução específica, a \textit{GPU} efetua a transição para a execução de outras \textit{threads} que não estão sujeitas a essa espera. Tal estratégia assegura que as unidades de processamento do \textit{GPU} estejam operando em sua máxima capacidade constantemente, resultando em uma elevada taxa de transferência. Na Figura \ref{fig_gpu_compute_arch}, é possível observar como as diferentes unidades de processamento dentro de uma \textit{GPU} são organizadas e como operam em paralelo.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.3]{./images/gpu_compute_arch.png}}
	\caption{\label{fig_gpu_compute_arch} Arquitetura do Modelo de computação das \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming}.}
\end{figure}


Internamente, a Unidade de Processamento Gráfico é composta por um conjunto de Multiprocessadores de Fluxo (\textit{SMs, Streaming Multiprocessors}) \citep{owens2007gpu}, com cada \textit{SM} possuindo uma quantidade de memória compartilhada. Esses \textit{SMs} são fundamentais para o desempenho paralelo da \textit{GPU}, permitindo o processamento simultâneo massivo. Por sua vez, \citeonline{wittenbrink2011fermi} esclarecem que cada \emph{SM} é composto por vários núcleos de processamento. Estes núcleos, frequentemente referidos como \textit{threads}, operam de maneira paralela. Importante destacar que dentro de cada \emph{SM}, todas essas \textit{threads} compartilham memória e outros recursos de processamento, facilitando a execução de tarefas paralelas de forma eficiente.


\subsubsection{Arquitetura Geral de Memória}
\label{subsec:gpu_mem_arch}

As \textit{GPUs} possuem uma hierarquia complexa de memórias, essencial para seu desempenho \citep{mei2016dissecting}. A \autoref{fig_gpu_mem_arch} ilustra a organização da memória em um \textit{SM}, destacando os registradores, que são privados a cada \textit{thread}. Os \textit{caches} de constantes armazenam dados constantes, exigindo que programadores declarem explicitamente objetos como constantes para otimização. Cada \textit{SM} tem uma memória compartilhada (\textit{scratchpad}) de baixa latência, ideal para reduzir acessos à memória global e sincronizar \textit{threads} \citep{owens2007gpu}. O cache L1 armazena dados frequentemente acessados do cache L2, que é compartilhado entre os \textit{SMs} \citep{picchi2015impact}, reduzindo a latência de acesso à memória global. Esses processos são transparentes para o \textit{SM}, semelhante aos caches em \textit{CPUs} \citep{mei2016dissecting}. A memória global externa ao chip, como a \textit{DRAM} de alta largura de banda encontrada na arquitetura \textit{Hopper} da Nvidia \citep{choquette2023nvidia}, apresenta alta latência. No entanto, as camadas adicionais de memória e o grande número de unidades de cálculo ajudam a minimizar esse impacto.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.15]{./images/gpu_mem_arch.jpeg}}
	\caption{\label{fig_gpu_mem_arch} Arquitetura de memória das \textit{GPUs}. Fonte: \citep{nvidiaCUDAProgramming}}
\end{figure}

A Memória Unificada permite que \textit{CPUs} e \textit{GPUs} acessem um mesmo endereço de memória \citep{nvidiaCUDAUnifiedMemory}. O driver \textit{CUDA} e o \textit{hardware} gerenciam automaticamente a migração das páginas de memória quando necessário \citeonline{chien2019performance}. Esse mecanismo é vantajoso para aplicações com padrões de acesso dispersos, evitando a necessidade de pré-carregar grandes arrays ou recorrer a acessos de alta latência fora do dispositivo (\textit{Zero Copy}). Com suporte a falhas de página, apenas as páginas necessárias são migradas, otimizando o desempenho.

\begin{figure}[!htb]
	\centerline{\includegraphics[scale=0.3]{./images/cuda_unified_mem.png}}
	\caption{\label{fig_gpu_mem_unified} Arquitetura de Memória Unificada CUDA. Fonte: \citep{nvidiaCUDAProgramming}}
\end{figure}

\section{Modelos de programação paralela em GPUs disponíveis no C++}\label{sec:gpu_models}

\subsection{Compute Shaders}\label{subsec:compute-shaders}

\subsection{CUDA/HIP}\label{subsec:cuda-hip}

\subsection{OpenCL}\label{subsec:opencl}

\subsection{OpenMP}\label{subsec:openmp}

\subsection{SYCL}\label{subsec:sycl}

\subsection{Bibliotecas de alto nível}\label{subsec:bibliotecas}

\subsection{stdexec}\label{subsec:sdtexec}


\section{stdexec: Modelo assíncrono/heterogêneo de execução de tarefas em GPUs}\label{sec:stdexec}

\bibliography{bibliography}

% you should really use BibTeX instead of this... :-)
% \begin{thebibliography}{99}

% \bibitem{bou91} Boulic, R. and Renault, O. (1991) ``3D Hierarchies for
% Animation'', In: New Trends in Animation and Visualization, Edited
% by Nadia Magnenat-Thalmann and Daniel Thalmann, John Wiley \& Sons
% ltd., England.
%
% \bibitem{dye95} Dyer, S., Martin, J. and Zulauf, J. (1995) ``Motion
% Capture White Paper'',
% http://reality.sgi.com/employees/jam\_sb/mocap/MoCapWP\_v2.0.html,
% December.
%
% \bibitem{hol95} Holton, M. and Alexander, S. (1995) ``Soft Cellular
% Modeling: A Technique for the Simulation of Non-rigid Materials'',
% Computer Graphics: Developments in Virtual Environments, R. A.
% Earnshaw and J. A. Vince, England, Academic Press Ltd., p.~449-460.
%
% \bibitem{knu84} Knuth, D. E., The TeXbook, Addison Wesley, 1984.
% \end{thebibliography}

\end{document}
